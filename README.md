# Adversarial Reinforcement Learning for Maze Generation

*Abstract*:

## Research Questions

1) How can adversarial reinforcement learning be used to generate high-quality mazes?
2) How does this approach compare to classical procedural maze algorithms (DFS, Prim’s, Kruskal’s)?
3) What reward functions encourage meaningful maze complexity?
4) How does coevolution between the Generator and Solver impact performance?

## Comparative Results

We evaluated our adversarial reinforcement learning approach through a comparative experiment against established maze 
generation techniques (Prim’s, Depth-First Search, and Random Maze Generation).  Mazes generated by our Deep Q-Network 
(DQN) and Proximal Policy Optimization (PPO) agents were assessed for solvability, difficulty, and triviality. We 
generated 100 mazes using each method, calculating metrics such as average steps to solution and solvability rate to 
benchmark performance. Implemented in Python with PyTorch and NumPy, the experiments leveraged a state space of size 
128x4 (for a 15x15 maze) and an action space of four cardinal directions (up, down, left, right), utilising a 
GPU-accelerated system for efficient training and generation.

| Method     | Maze Size | Average Steps to Solution | Solvability Rate $\uparrow$ | Average Triviality $\uparrow$ | Average Difficulty $\uparrow$ |
|------------|-----------|---------------------------|-----------------------------|-------------------------------|-------------------------------|
| DFS        | 15        | 42.940                    | 1.000                       | 4.630                         | 32.208                        |
| Prim       | 15        | 13.540                    | 1.000                       | 10.020                        | 37.848                        |
| RMG        | 15        | 8.000                     | 0.030                       | 7.190                         | 30.798                        |
| DQN (Ours) | 15        | 5.429                     | 0.070                       | 4.630                         | 50.337                        |
| PPO (Ours) | 15        | 4.800                     | 0.100                       | 17.000                        | 47.574                        | 

- DFS $\rightarrow$ Depth First Search Algorithm
- RMG $\rightarrow$ Random Maze Generator Policy
- DQN $\rightarrow$ Deep Q-Learning Model
- PPO $\rightarrow$ Proximal Policy Optimisation Model

A higher triviality score indicates a less trivial the maze. Higher difficulty score indicates a more difficult maze.

## Installation
It is recommended that you have Python 3.13 or greater.

#### Rocm:
```commandline
conda env create --name adversarial-maze-rl -f environment.yml
```

#### Nvidia:
```commandline
conda env create --name adversarial-maze-rl -f environment.yml
conda activate adversarial-maze-rl
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
```

## How to train the models

#### DQN

```commandline
 python train_DQN.py --gen_episode_num 70 --sol_episode_num 20 --train_iters 100 
```

Options:
  - -h, --help            show this help message and exit
  - --gen_episode_num GEN_EPISODE_NUM
                        Number of generator training episodes.
  - --sol_episode_num SOL_EPISODE_NUM
                        Number of solver training episodes.
  - --train_iters TRAIN_ITERS
                        Number of alternated training rounds.
  - --replay_size REPLAY_SIZE
                        Replay buffer size.
  - --mini_batch_size MINI_BATCH_SIZE
                        Mini-batch size for solver.
  - --maze_size MAZE_SIZE
                        Size of the maze.
  - --lr LR               Learning rate.


#### PPO

```commandline
python train_PPO.py --train_iters 100 --gen_episode_num 50 --sol_episode_num 20
```
Options:
  - -h, --help            show this help message and exit
  - --gen_episode_num GEN_EPISODE_NUM
                        Number of generator training episodes.
  - --sol_episode_num SOL_EPISODE_NUM
                        Number of solver training episodes per maze.
  - --train_iters TRAIN_ITERS
                        Number of alternated training rounds.
  - --maze_size MAZE_SIZE
                        Size of the maze.
  - --gen_lr GEN_LR       Generator learning rate.
  - --sol_lr SOL_LR       Solver learning rate.

> The results from training (weights, sample maze images, and plots) will be available in the default `./runs` directory.

## How to evaluate the models

```commandline
python eval_visualise.py --dqn_weights <path to the DQN weights> --ppo_weights <path to PPO weights>

Example: python eval_visualise.py --dqn_weights runs/exp_2025-04-30_DQN/weights/generator_ep100.pth --ppo_weights runs/exp_2025-04-28_PPO_100_iter/weights/generator_ep_100.pth
```

## Future Works

- Incorporate constraints for maze generation.
- Experiment with different Adversarial training ratios.
- Generate mazes on a spectrum rather than only binary.
- Create a more robust reward system to encourage maze generation complexity.

# Troubleshooting

All common issues will be posted here with their corresponding solutions. If you run into any errors, feel free to open an issue. We'll do our best to help you resolve it.



